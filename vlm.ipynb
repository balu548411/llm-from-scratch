{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Vision-Language Model (VLM) Pretraining Script\n",
    "===============================================\n",
    "Modern implementation for pretraining vision-language models from scratch:\n",
    "- Vision Transformer (ViT) for image encoding\n",
    "- Cross-attention mechanism for vision-language fusion\n",
    "- Flash Attention 2 for efficient attention\n",
    "- Mixed precision training (bfloat16)\n",
    "- Contrastive learning objectives (CLIP-style)\n",
    "- Next-token prediction for language generation\n",
    "- Q-Former architecture for efficient vision-language alignment\n",
    "\n",
    "Dataset: Will use a popular VLM dataset from HuggingFace\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.distributed as dist\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "from transformers import PreTrainedTokenizerFast, AutoTokenizer\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Dict, Any, Tuple\n",
    "import wandb\n",
    "from tqdm.auto import tqdm\n",
    "import logging\n",
    "from io import BytesIO\n",
    "import requests\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n"
   ],
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "@dataclass\n",
    "class VLMConfig:\n",
    "    \"\"\"Configuration for the Vision-Language Model.\"\"\"\n",
    "\n",
    "    # Vision encoder (ViT)\n",
    "    image_size: int = 224\n",
    "    patch_size: int = 14\n",
    "    num_channels: int = 3\n",
    "    vision_hidden_size: int = 1024\n",
    "    vision_num_layers: int = 24\n",
    "    vision_num_heads: int = 16\n",
    "    vision_intermediate_size: int = 4096\n",
    "\n",
    "    # Q-Former (Query Transformer for vision-language alignment)\n",
    "    num_query_tokens: int = 32\n",
    "    qformer_hidden_size: int = 768\n",
    "    qformer_num_layers: int = 12\n",
    "    qformer_num_heads: int = 12\n",
    "\n",
    "    # Language model\n",
    "    vocab_size: int = 50304\n",
    "    max_seq_length: int = 512\n",
    "    hidden_size: int = 2048\n",
    "    num_hidden_layers: int = 24\n",
    "    num_attention_heads: int = 16\n",
    "    num_key_value_heads: int = 8\n",
    "    intermediate_size: int = 5632\n",
    "\n",
    "    # General\n",
    "    hidden_act: str = \"silu\"\n",
    "    rms_norm_eps: float = 1e-6\n",
    "    rope_theta: float = 10000.0\n",
    "    attention_dropout: float = 0.0\n",
    "    use_flash_attention: bool = True\n",
    "    gradient_checkpointing: bool = True\n",
    "\n",
    "    # Vision-Language fusion\n",
    "    projection_dim: int = 512  # For contrastive learning\n"
   ],
   "id": "51cf31854ac5bd81",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "@dataclass\n",
    "class VLMTrainingConfig:\n",
    "    \"\"\"Configuration for VLM training.\"\"\"\n",
    "\n",
    "    # Data\n",
    "    dataset_name: str = \"HuggingFaceM4/COCO\"  # Multi-modal COCO dataset\n",
    "    dataset_split: str = \"train\"\n",
    "    image_size: int = 224\n",
    "    max_seq_length: int = 512\n",
    "\n",
    "    # Training\n",
    "    batch_size: int = 8\n",
    "    gradient_accumulation_steps: int = 16  # Effective batch size = 128\n",
    "    num_epochs: int = 3\n",
    "    max_steps: Optional[int] = None\n",
    "\n",
    "    # Optimization\n",
    "    learning_rate: float = 1e-4\n",
    "    vision_lr: float = 5e-5  # Lower LR for vision encoder\n",
    "    weight_decay: float = 0.05\n",
    "    adam_beta1: float = 0.9\n",
    "    adam_beta2: float = 0.999\n",
    "    adam_epsilon: float = 1e-8\n",
    "    max_grad_norm: float = 1.0\n",
    "    warmup_steps: int = 1000\n",
    "\n",
    "    # Loss weights\n",
    "    contrastive_loss_weight: float = 1.0\n",
    "    language_loss_weight: float = 1.0\n",
    "\n",
    "    # Mixed precision\n",
    "    use_fp16: bool = False\n",
    "    use_bf16: bool = True\n",
    "\n",
    "    # Checkpointing\n",
    "    save_steps: int = 5000\n",
    "    output_dir: str = \"./checkpoints/vlm\"\n",
    "\n",
    "    # Logging\n",
    "    logging_steps: int = 10\n",
    "    use_wandb: bool = False\n",
    "    wandb_project: str = \"vlm-pretraining\"\n",
    "\n",
    "    # Tokenizer\n",
    "    tokenizer_path: str = \"./tokenizer\"  # From LLM pretraining\n",
    "\n",
    "    # Distributed\n",
    "    local_rank: int = -1\n",
    "    world_size: int = 1\n"
   ],
   "id": "6602135c740e34e0",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Convert images to patch embeddings.\n",
    "    Splits image into non-overlapping patches and projects them.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: VLMConfig):\n",
    "        \"\"\"\n",
    "        Initialize patch embedding layer.\n",
    "\n",
    "        Args:\n",
    "            config: VLM configuration\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.image_size = config.image_size\n",
    "        self.patch_size = config.patch_size\n",
    "        self.num_patches = (config.image_size // config.patch_size) ** 2\n",
    "\n",
    "        # Convolutional projection\n",
    "        self.projection = nn.Conv2d(\n",
    "            config.num_channels,\n",
    "            config.vision_hidden_size,\n",
    "            kernel_size=config.patch_size,\n",
    "            stride=config.patch_size,\n",
    "        )\n",
    "\n",
    "    def forward(self, pixel_values: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Convert images to patch embeddings.\n",
    "\n",
    "        Args:\n",
    "            pixel_values: Image tensor of shape (batch, channels, height, width)\n",
    "\n",
    "        Returns:\n",
    "            Patch embeddings of shape (batch, num_patches, hidden_size)\n",
    "        \"\"\"\n",
    "        # Project and flatten\n",
    "        embeddings = self.projection(pixel_values)  # (batch, hidden_size, H, W)\n",
    "        embeddings = embeddings.flatten(2).transpose(1, 2)  # (batch, num_patches, hidden_size)\n",
    "        return embeddings\n"
   ],
   "id": "c5ab659aa375d6ec",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "class VisionTransformerEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Vision Transformer (ViT) encoder for processing images.\n",
    "    Based on \"An Image is Worth 16x16 Words\" paper.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: VLMConfig):\n",
    "        \"\"\"\n",
    "        Initialize ViT encoder.\n",
    "\n",
    "        Args:\n",
    "            config: VLM configuration\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        # Patch embedding\n",
    "        self.patch_embedding = PatchEmbedding(config)\n",
    "\n",
    "        # CLS token\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, config.vision_hidden_size))\n",
    "\n",
    "        # Position embeddings\n",
    "        num_positions = self.patch_embedding.num_patches + 1  # +1 for CLS token\n",
    "        self.position_embeddings = nn.Parameter(torch.zeros(1, num_positions, config.vision_hidden_size))\n",
    "\n",
    "        # Transformer layers\n",
    "        self.layers = nn.ModuleList([\n",
    "            VisionTransformerLayer(config) for _ in range(config.vision_num_layers)\n",
    "        ])\n",
    "\n",
    "        self.layernorm = nn.LayerNorm(config.vision_hidden_size, eps=1e-6)\n",
    "\n",
    "    def forward(self, pixel_values: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Encode images.\n",
    "\n",
    "        Args:\n",
    "            pixel_values: Image tensor of shape (batch, channels, height, width)\n",
    "\n",
    "        Returns:\n",
    "            Encoded features of shape (batch, num_patches+1, hidden_size)\n",
    "        \"\"\"\n",
    "        batch_size = pixel_values.shape[0]\n",
    "\n",
    "        # Get patch embeddings\n",
    "        embeddings = self.patch_embedding(pixel_values)\n",
    "\n",
    "        # Add CLS token\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
    "        embeddings = torch.cat([cls_tokens, embeddings], dim=1)\n",
    "\n",
    "        # Add position embeddings\n",
    "        embeddings = embeddings + self.position_embeddings\n",
    "\n",
    "        # Pass through transformer layers\n",
    "        for layer in self.layers:\n",
    "            embeddings = layer(embeddings)\n",
    "\n",
    "        embeddings = self.layernorm(embeddings)\n",
    "        return embeddings\n"
   ],
   "id": "bd8d1a992e951a97",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "class VisionTransformerLayer(nn.Module):\n",
    "    \"\"\"Single ViT transformer layer.\"\"\"\n",
    "\n",
    "    def __init__(self, config: VLMConfig):\n",
    "        \"\"\"\n",
    "        Initialize ViT layer.\n",
    "\n",
    "        Args:\n",
    "            config: VLM configuration\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            config.vision_hidden_size,\n",
    "            config.vision_num_heads,\n",
    "            dropout=config.attention_dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(config.vision_hidden_size, config.vision_intermediate_size),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(config.vision_intermediate_size, config.vision_hidden_size),\n",
    "        )\n",
    "        self.layernorm1 = nn.LayerNorm(config.vision_hidden_size, eps=1e-6)\n",
    "        self.layernorm2 = nn.LayerNorm(config.vision_hidden_size, eps=1e-6)\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass with residual connections.\n",
    "\n",
    "        Args:\n",
    "            hidden_states: Input tensor\n",
    "\n",
    "        Returns:\n",
    "            Output tensor\n",
    "        \"\"\"\n",
    "        # Self-attention\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.layernorm1(hidden_states)\n",
    "        hidden_states, _ = self.attention(hidden_states, hidden_states, hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "\n",
    "        # MLP\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.layernorm2(hidden_states)\n",
    "        hidden_states = self.mlp(hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "\n",
    "        return hidden_states\n"
   ],
   "id": "c3f5783bf02daafe",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "class QFormerLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Q-Former layer for vision-language alignment.\n",
    "    Uses learnable query tokens to extract relevant visual features.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: VLMConfig):\n",
    "        \"\"\"\n",
    "        Initialize Q-Former layer.\n",
    "\n",
    "        Args:\n",
    "            config: VLM configuration\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.hidden_size = config.qformer_hidden_size\n",
    "\n",
    "        # Self-attention on queries\n",
    "        self.self_attention = nn.MultiheadAttention(\n",
    "            config.qformer_hidden_size,\n",
    "            config.qformer_num_heads,\n",
    "            dropout=config.attention_dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        # Cross-attention with vision features\n",
    "        self.cross_attention = nn.MultiheadAttention(\n",
    "            config.qformer_hidden_size,\n",
    "            config.qformer_num_heads,\n",
    "            dropout=config.attention_dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        # MLP\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(config.qformer_hidden_size, config.qformer_hidden_size * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(config.qformer_hidden_size * 4, config.qformer_hidden_size),\n",
    "        )\n",
    "\n",
    "        self.layernorm1 = nn.LayerNorm(config.qformer_hidden_size, eps=1e-6)\n",
    "        self.layernorm2 = nn.LayerNorm(config.qformer_hidden_size, eps=1e-6)\n",
    "        self.layernorm3 = nn.LayerNorm(config.qformer_hidden_size, eps=1e-6)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        query_embeds: torch.Tensor,\n",
    "        encoder_hidden_states: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of Q-Former layer.\n",
    "\n",
    "        Args:\n",
    "            query_embeds: Query embeddings (learnable)\n",
    "            encoder_hidden_states: Vision encoder outputs\n",
    "\n",
    "        Returns:\n",
    "            Updated query embeddings\n",
    "        \"\"\"\n",
    "        # Self-attention\n",
    "        residual = query_embeds\n",
    "        query_embeds = self.layernorm1(query_embeds)\n",
    "        query_embeds, _ = self.self_attention(query_embeds, query_embeds, query_embeds)\n",
    "        query_embeds = residual + query_embeds\n",
    "\n",
    "        # Cross-attention with vision features\n",
    "        residual = query_embeds\n",
    "        query_embeds = self.layernorm2(query_embeds)\n",
    "        query_embeds, _ = self.cross_attention(\n",
    "            query_embeds,\n",
    "            encoder_hidden_states,\n",
    "            encoder_hidden_states,\n",
    "        )\n",
    "        query_embeds = residual + query_embeds\n",
    "\n",
    "        # MLP\n",
    "        residual = query_embeds\n",
    "        query_embeds = self.layernorm3(query_embeds)\n",
    "        query_embeds = self.mlp(query_embeds)\n",
    "        query_embeds = residual + query_embeds\n",
    "\n",
    "        return query_embeds\n"
   ],
   "id": "dcfd86d5c85ef15d",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "class QFormer(nn.Module):\n",
    "    \"\"\"\n",
    "    Q-Former module for bridging vision and language.\n",
    "    Introduced in BLIP-2 paper: https://arxiv.org/abs/2301.12597\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: VLMConfig):\n",
    "        \"\"\"\n",
    "        Initialize Q-Former.\n",
    "\n",
    "        Args:\n",
    "            config: VLM configuration\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        # Learnable query tokens\n",
    "        self.query_tokens = nn.Parameter(\n",
    "            torch.zeros(1, config.num_query_tokens, config.qformer_hidden_size)\n",
    "        )\n",
    "        nn.init.normal_(self.query_tokens, std=0.02)\n",
    "\n",
    "        # Vision to Q-Former projection\n",
    "        self.vision_proj = nn.Linear(config.vision_hidden_size, config.qformer_hidden_size)\n",
    "\n",
    "        # Q-Former layers\n",
    "        self.layers = nn.ModuleList([\n",
    "            QFormerLayer(config) for _ in range(config.qformer_num_layers)\n",
    "        ])\n",
    "\n",
    "        # Output projection to language model dimension\n",
    "        self.proj_to_language = nn.Linear(config.qformer_hidden_size, config.hidden_size)\n",
    "\n",
    "    def forward(self, vision_outputs: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Extract visual features using learnable queries.\n",
    "\n",
    "        Args:\n",
    "            vision_outputs: Vision encoder outputs of shape (batch, seq_len, vision_hidden_size)\n",
    "\n",
    "        Returns:\n",
    "            Query outputs of shape (batch, num_queries, hidden_size)\n",
    "        \"\"\"\n",
    "        batch_size = vision_outputs.shape[0]\n",
    "\n",
    "        # Project vision features\n",
    "        vision_outputs = self.vision_proj(vision_outputs)\n",
    "\n",
    "        # Expand query tokens\n",
    "        query_embeds = self.query_tokens.expand(batch_size, -1, -1)\n",
    "\n",
    "        # Pass through Q-Former layers\n",
    "        for layer in self.layers:\n",
    "            query_embeds = layer(query_embeds, vision_outputs)\n",
    "\n",
    "        # Project to language model dimension\n",
    "        query_outputs = self.proj_to_language(query_embeds)\n",
    "\n",
    "        return query_outputs\n"
   ],
   "id": "6de085a06f2645a3",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "class VisionLanguageModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete Vision-Language Model combining:\n",
    "    - Vision Transformer for image encoding\n",
    "    - Q-Former for vision-language alignment\n",
    "    - Language model for text generation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: VLMConfig):\n",
    "        \"\"\"\n",
    "        Initialize VLM.\n",
    "\n",
    "        Args:\n",
    "            config: VLM configuration\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        # Vision encoder\n",
    "        self.vision_encoder = VisionTransformerEncoder(config)\n",
    "\n",
    "        # Q-Former for vision-language alignment\n",
    "        self.qformer = QFormer(config)\n",
    "\n",
    "        # Language model components (simplified decoder)\n",
    "        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerDecoderLayer(config) for _ in range(config.num_hidden_layers)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "\n",
    "        # Contrastive learning heads\n",
    "        self.vision_projection = nn.Linear(config.vision_hidden_size, config.projection_dim)\n",
    "        self.text_projection = nn.Linear(config.hidden_size, config.projection_dim)\n",
    "\n",
    "        # Temperature parameter for contrastive loss\n",
    "        self.logit_scale = nn.Parameter(torch.ones([]) * math.log(1 / 0.07))\n",
    "\n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"Initialize weights.\"\"\"\n",
    "        std = 0.02\n",
    "        if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
    "            module.weight.data.normal_(mean=0.0, std=std)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=std)\n",
    "\n",
    "    def encode_image(self, pixel_values: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Encode images through vision encoder and Q-Former.\n",
    "\n",
    "        Args:\n",
    "            pixel_values: Image tensor\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (vision_embeds for contrastive, query_outputs for generation)\n",
    "        \"\"\"\n",
    "        # Encode image\n",
    "        vision_outputs = self.vision_encoder(pixel_values)\n",
    "\n",
    "        # Get CLS token for contrastive learning\n",
    "        vision_embeds = vision_outputs[:, 0, :]  # CLS token\n",
    "\n",
    "        # Get query outputs for language generation\n",
    "        query_outputs = self.qformer(vision_outputs)\n",
    "\n",
    "        return vision_embeds, query_outputs\n",
    "\n",
    "    def encode_text(self, input_ids: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Encode text for contrastive learning.\n",
    "\n",
    "        Args:\n",
    "            input_ids: Token IDs\n",
    "\n",
    "        Returns:\n",
    "            Text embeddings\n",
    "        \"\"\"\n",
    "        # Embed tokens\n",
    "        hidden_states = self.embed_tokens(input_ids)\n",
    "\n",
    "        # Pass through transformer (simplified - just use first layer)\n",
    "        for layer in self.layers[:4]:  # Use first few layers for efficiency\n",
    "            hidden_states = layer(hidden_states, encoder_hidden_states=None)\n",
    "\n",
    "        # Pool (mean pooling)\n",
    "        text_embeds = hidden_states.mean(dim=1)\n",
    "\n",
    "        return text_embeds\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        pixel_values: torch.Tensor,\n",
    "        input_ids: torch.Tensor,\n",
    "        labels: Optional[torch.Tensor] = None,\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass computing both contrastive and generation losses.\n",
    "\n",
    "        Args:\n",
    "            pixel_values: Image tensor\n",
    "            input_ids: Token IDs\n",
    "            labels: Optional labels for language modeling\n",
    "\n",
    "        Returns:\n",
    "            Dictionary containing losses and logits\n",
    "        \"\"\"\n",
    "        batch_size = pixel_values.shape[0]\n",
    "\n",
    "        # Encode images\n",
    "        vision_embeds, query_outputs = self.encode_image(pixel_values)\n",
    "\n",
    "        # Encode text\n",
    "        text_embeds = self.encode_text(input_ids)\n",
    "\n",
    "        # Project for contrastive learning\n",
    "        vision_features = F.normalize(self.vision_projection(vision_embeds), dim=-1)\n",
    "        text_features = F.normalize(self.text_projection(text_embeds), dim=-1)\n",
    "\n",
    "        # Compute contrastive loss (CLIP-style)\n",
    "        logit_scale = self.logit_scale.exp()\n",
    "        logits_per_image = logit_scale * vision_features @ text_features.t()\n",
    "        logits_per_text = logits_per_image.t()\n",
    "\n",
    "        # Contrastive labels (diagonal)\n",
    "        contrastive_labels = torch.arange(batch_size, device=pixel_values.device)\n",
    "\n",
    "        contrastive_loss = (\n",
    "            F.cross_entropy(logits_per_image, contrastive_labels) +\n",
    "            F.cross_entropy(logits_per_text, contrastive_labels)\n",
    "        ) / 2\n",
    "\n",
    "        # Language modeling\n",
    "        # Combine visual queries with text embeddings\n",
    "        text_embeddings = self.embed_tokens(input_ids)\n",
    "        combined_embeds = torch.cat([query_outputs, text_embeddings], dim=1)\n",
    "\n",
    "        # Pass through language model\n",
    "        hidden_states = combined_embeds\n",
    "        for layer in self.layers:\n",
    "            hidden_states = layer(hidden_states, encoder_hidden_states=None)\n",
    "\n",
    "        hidden_states = self.norm(hidden_states)\n",
    "        logits = self.lm_head(hidden_states)\n",
    "\n",
    "        # Compute language modeling loss\n",
    "        language_loss = None\n",
    "        if labels is not None:\n",
    "            # Shift for next-token prediction (skip visual tokens)\n",
    "            num_visual_tokens = query_outputs.shape[1]\n",
    "            shift_logits = logits[:, num_visual_tokens:-1, :].contiguous()\n",
    "            shift_labels = labels[:, 1:].contiguous()\n",
    "\n",
    "            language_loss = F.cross_entropy(\n",
    "                shift_logits.view(-1, shift_logits.size(-1)),\n",
    "                shift_labels.view(-1),\n",
    "                ignore_index=-100,\n",
    "            )\n",
    "\n",
    "        return {\n",
    "            \"contrastive_loss\": contrastive_loss,\n",
    "            \"language_loss\": language_loss,\n",
    "            \"logits\": logits,\n",
    "        }\n"
   ],
   "id": "41a8892c3544fb43",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "class TransformerDecoderLayer(nn.Module):\n",
    "    \"\"\"Simplified transformer decoder layer for language modeling.\"\"\"\n",
    "\n",
    "    def __init__(self, config: VLMConfig):\n",
    "        \"\"\"Initialize decoder layer.\"\"\"\n",
    "        super().__init__()\n",
    "        self.self_attention = nn.MultiheadAttention(\n",
    "            config.hidden_size,\n",
    "            config.num_attention_heads,\n",
    "            dropout=config.attention_dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(config.hidden_size, config.intermediate_size),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(config.intermediate_size, config.hidden_size),\n",
    "        )\n",
    "        self.layernorm1 = nn.LayerNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        self.layernorm2 = nn.LayerNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        encoder_hidden_states: Optional[torch.Tensor] = None,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Forward pass.\"\"\"\n",
    "        # Self-attention\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.layernorm1(hidden_states)\n",
    "        hidden_states, _ = self.self_attention(hidden_states, hidden_states, hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "\n",
    "        # MLP\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.layernorm2(hidden_states)\n",
    "        hidden_states = self.mlp(hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "\n",
    "        return hidden_states"
   ],
   "id": "5aa82308d329d127",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "\n",
    "class VisionLanguageDataset(Dataset):\n",
    "    \"\"\"Dataset for vision-language pretraining.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset_name: str,\n",
    "        tokenizer: PreTrainedTokenizerFast,\n",
    "        image_size: int = 224,\n",
    "        max_length: int = 512,\n",
    "        split: str = \"train\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize VL dataset.\n",
    "\n",
    "        Args:\n",
    "            dataset_name: HuggingFace dataset name\n",
    "            tokenizer: Tokenizer instance\n",
    "            image_size: Size to resize images\n",
    "            max_length: Maximum text sequence length\n",
    "            split: Dataset split\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "        # Image transforms\n",
    "        self.transform = T.Compose([\n",
    "            T.Resize((image_size, image_size)),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "\n",
    "        logger.info(f\"Loading dataset {dataset_name}...\")\n",
    "        self.dataset = load_dataset(dataset_name, split=split, streaming=False)\n",
    "        logger.info(f\"Dataset loaded with {len(self.dataset)} samples\")\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Get a single image-text pair.\n",
    "\n",
    "        Args:\n",
    "            idx: Sample index\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with pixel_values, input_ids, and labels\n",
    "        \"\"\"\n",
    "        sample = self.dataset[idx]\n",
    "\n",
    "        # Process image\n",
    "        image = sample[\"image\"]\n",
    "        if isinstance(image, str):\n",
    "            # Load image from URL or path\n",
    "            try:\n",
    "                if image.startswith(\"http\"):\n",
    "                    image = Image.open(BytesIO(requests.get(image).content)).convert(\"RGB\")\n",
    "                else:\n",
    "                    image = Image.open(image).convert(\"RGB\")\n",
    "            except:\n",
    "                # Fallback to blank image\n",
    "                image = Image.new(\"RGB\", (224, 224))\n",
    "\n",
    "        pixel_values = self.transform(image)\n",
    "\n",
    "        # Process text (use sentences/captions field based on dataset structure)\n",
    "        text = sample.get(\"sentences\", sample.get(\"caption\", \"\"))\n",
    "        if isinstance(text, list):\n",
    "            text = \" \".join([s[\"raw\"] if isinstance(s, dict) else s for s in text])\n",
    "\n",
    "        # Tokenize\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        input_ids = encoding[\"input_ids\"].squeeze(0)\n",
    "\n",
    "        return {\n",
    "            \"pixel_values\": pixel_values,\n",
    "            \"input_ids\": input_ids,\n",
    "            \"labels\": input_ids.clone(),\n",
    "        }\n"
   ],
   "id": "19d7ea2123ca6deb",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "def train_vlm(config: VLMTrainingConfig):\n",
    "    \"\"\"\n",
    "    Main VLM training function.\n",
    "\n",
    "    Args:\n",
    "        config: Training configuration\n",
    "    \"\"\"\n",
    "    # Setup distributed\n",
    "    rank, world_size, local_rank = 0, 1, -1\n",
    "    if \"RANK\" in os.environ:\n",
    "        rank = int(os.environ[\"RANK\"])\n",
    "        world_size = int(os.environ[\"WORLD_SIZE\"])\n",
    "        local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "        dist.init_process_group(\"nccl\")\n",
    "        torch.cuda.set_device(local_rank)\n",
    "\n",
    "    is_main_process = rank == 0\n",
    "\n",
    "    if is_main_process:\n",
    "        logger.info(\"Starting VLM pretraining...\")\n",
    "        if config.use_wandb:\n",
    "            wandb.init(project=config.wandb_project, config=config.__dict__)\n",
    "\n",
    "    # Device\n",
    "    device = torch.device(f\"cuda:{local_rank}\" if local_rank >= 0 else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Load tokenizer\n",
    "    tokenizer = PreTrainedTokenizerFast.from_pretrained(config.tokenizer_path)\n",
    "\n",
    "    # Create model\n",
    "    model_config = VLMConfig(vocab_size=len(tokenizer))\n",
    "    model = VisionLanguageModel(model_config).to(device)\n",
    "\n",
    "    if is_main_process:\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        logger.info(f\"Model parameters: {total_params:,} ({total_params / 1e9:.2f}B)\")\n",
    "\n",
    "    # DDP\n",
    "    if world_size > 1:\n",
    "        model = DDP(model, device_ids=[local_rank])\n",
    "\n",
    "    # Dataset\n",
    "    dataset = VisionLanguageDataset(\n",
    "        config.dataset_name,\n",
    "        tokenizer,\n",
    "        config.image_size,\n",
    "        config.max_seq_length,\n",
    "        config.dataset_split,\n",
    "    )\n",
    "    sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank) if world_size > 1 else None\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=config.batch_size,\n",
    "        sampler=sampler,\n",
    "        shuffle=(sampler is None),\n",
    "        num_workers=4,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    # Optimizer with different learning rates for vision and language\n",
    "    vision_params = list(model.vision_encoder.parameters()) if not hasattr(model, \"module\") else list(model.module.vision_encoder.parameters())\n",
    "    other_params = [p for n, p in model.named_parameters() if \"vision_encoder\" not in n]\n",
    "\n",
    "    optimizer = torch.optim.AdamW([\n",
    "        {\"params\": vision_params, \"lr\": config.vision_lr},\n",
    "        {\"params\": other_params, \"lr\": config.learning_rate},\n",
    "    ], weight_decay=config.weight_decay, betas=(config.adam_beta1, config.adam_beta2))\n",
    "\n",
    "    # Scheduler\n",
    "    total_steps = config.max_steps if config.max_steps else len(dataloader) * config.num_epochs // config.gradient_accumulation_steps\n",
    "    scheduler = get_cosine_schedule_with_warmup(optimizer, config.warmup_steps, total_steps)\n",
    "\n",
    "    # Mixed precision\n",
    "    scaler = torch.cuda.amp.GradScaler() if config.use_fp16 else None\n",
    "    dtype = torch.bfloat16 if config.use_bf16 else torch.float16 if config.use_fp16 else torch.float32\n",
    "\n",
    "    # Training loop\n",
    "    global_step = 0\n",
    "    model.zero_grad()\n",
    "\n",
    "    for epoch in range(config.num_epochs):\n",
    "        if sampler:\n",
    "            sampler.set_epoch(epoch)\n",
    "\n",
    "        progress_bar = tqdm(dataloader, disable=not is_main_process)\n",
    "\n",
    "        for step, batch in enumerate(progress_bar):\n",
    "            pixel_values = batch[\"pixel_values\"].to(device)\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            with torch.cuda.amp.autocast(dtype=dtype):\n",
    "                outputs = model(pixel_values, input_ids, labels)\n",
    "\n",
    "                # Combined loss\n",
    "                loss = (\n",
    "                    config.contrastive_loss_weight * outputs[\"contrastive_loss\"] +\n",
    "                    config.language_loss_weight * outputs[\"language_loss\"]\n",
    "                ) / config.gradient_accumulation_steps\n",
    "\n",
    "            # Backward\n",
    "            if scaler:\n",
    "                scaler.scale(loss).backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "\n",
    "            # Update\n",
    "            if (step + 1) % config.gradient_accumulation_steps == 0:\n",
    "                if scaler:\n",
    "                    scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), config.max_grad_norm)\n",
    "\n",
    "                if scaler:\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                else:\n",
    "                    optimizer.step()\n",
    "\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                global_step += 1\n",
    "\n",
    "                # Logging\n",
    "                if global_step % config.logging_steps == 0 and is_main_process:\n",
    "                    loss_val = loss.item() * config.gradient_accumulation_steps\n",
    "                    progress_bar.set_postfix({\"loss\": f\"{loss_val:.4f}\"})\n",
    "\n",
    "                    if config.use_wandb:\n",
    "                        wandb.log({\n",
    "                            \"train/total_loss\": loss_val,\n",
    "                            \"train/contrastive_loss\": outputs[\"contrastive_loss\"].item(),\n",
    "                            \"train/language_loss\": outputs[\"language_loss\"].item(),\n",
    "                            \"train/learning_rate\": scheduler.get_last_lr()[0],\n",
    "                            \"train/step\": global_step,\n",
    "                        })\n",
    "\n",
    "                # Save checkpoint\n",
    "                if global_step % config.save_steps == 0 and is_main_process:\n",
    "                    checkpoint_path = os.path.join(config.output_dir, f\"checkpoint-{global_step}\")\n",
    "                    os.makedirs(checkpoint_path, exist_ok=True)\n",
    "\n",
    "                    model_to_save = model.module if hasattr(model, \"module\") else model\n",
    "                    torch.save({\n",
    "                        \"model\": model_to_save.state_dict(),\n",
    "                        \"optimizer\": optimizer.state_dict(),\n",
    "                        \"scheduler\": scheduler.state_dict(),\n",
    "                        \"step\": global_step,\n",
    "                        \"config\": model_config,\n",
    "                    }, os.path.join(checkpoint_path, \"pytorch_model.bin\"))\n",
    "\n",
    "                    logger.info(f\"Checkpoint saved at step {global_step}\")\n",
    "\n",
    "                if config.max_steps and global_step >= config.max_steps:\n",
    "                    break\n",
    "\n",
    "        if config.max_steps and global_step >= config.max_steps:\n",
    "            break\n",
    "\n",
    "    # Save final model\n",
    "    if is_main_process:\n",
    "        final_path = os.path.join(config.output_dir, \"final\")\n",
    "        os.makedirs(final_path, exist_ok=True)\n",
    "\n",
    "        model_to_save = model.module if hasattr(model, \"module\") else model\n",
    "        torch.save({\n",
    "            \"model\": model_to_save.state_dict(),\n",
    "            \"config\": model_config,\n",
    "        }, os.path.join(final_path, \"pytorch_model.bin\"))\n",
    "\n",
    "        logger.info(f\"Final model saved to {final_path}\")\n",
    "\n",
    "        if config.use_wandb:\n",
    "            wandb.finish()\n",
    "\n",
    "    if world_size > 1:\n",
    "        dist.destroy_process_group()\n"
   ],
   "id": "a336b7158a4c5d72",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "config = VLMTrainingConfig()\n",
    "train_vlm(config)\n",
    "\n"
   ],
   "id": "14aae33c57f58465"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "654e2139b759f48c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
