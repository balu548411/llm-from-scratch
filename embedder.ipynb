{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "BERT-style Bidirectional Encoder for Embedding Generation\n",
    "\n",
    "This module implements an encoder-only transformer architecture similar to BERT\n",
    "for learning text embeddings. The model supports both Masked Language Modeling (MLM)\n",
    "and contrastive learning objectives.\n",
    "\n",
    "Architecture:\n",
    "    - Token + Position Embeddings\n",
    "    - N Transformer Encoder Blocks (bidirectional)\n",
    "    - Pooling Layer (mean/cls/max)\n",
    "    - MLM Head (optional)\n",
    "    - Contrastive Learning Support\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Optional, Tuple, Dict"
   ],
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head self-attention without causal masking (bidirectional).\n",
    "\n",
    "    Unlike the decoder-only model, this attention mechanism allows each position\n",
    "    to attend to all positions in the sequence, enabling bidirectional context.\n",
    "\n",
    "    Args:\n",
    "        hidden_size (int): Dimension of input embeddings\n",
    "        num_attention_heads (int): Number of parallel attention heads\n",
    "        dropout (float): Dropout probability\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_size: int, num_attention_heads: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        assert hidden_size % num_attention_heads == 0, \"hidden_size must be divisible by num_attention_heads\"\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.head_dim = hidden_size // num_attention_heads\n",
    "        self.scale = math.sqrt(self.head_dim)\n",
    "\n",
    "        # Combined QKV projection\n",
    "        self.qkv_proj = nn.Linear(hidden_size, 3 * hidden_size)\n",
    "        self.out_proj = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.resid_dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass with bidirectional attention.\n",
    "\n",
    "        Args:\n",
    "            hidden_states: Input (batch_size, seq_len, hidden_size)\n",
    "            attention_mask: Optional mask (batch_size, 1, 1, seq_len)\n",
    "\n",
    "        Returns:\n",
    "            Output tensor (batch_size, seq_len, hidden_size)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = hidden_states.size()\n",
    "\n",
    "        # Project and split into Q, K, V\n",
    "        qkv = self.qkv_proj(hidden_states)\n",
    "        qkv = qkv.reshape(batch_size, seq_len, 3, self.num_attention_heads, self.head_dim) # (batch, seq_len, 3, heads, head_dim)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4) # (3, batch, heads, seq_len, head_dim)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        # Compute attention scores\n",
    "        attn_weights = torch.matmul(q, k.transpose(-2, -1)) / self.scale\n",
    "\n",
    "        # Apply padding mask if provided (no causal masking)\n",
    "        if attention_mask is not None:\n",
    "            attn_weights = attn_weights.masked_fill(attention_mask == 0, float('-inf'))\n",
    "\n",
    "        attn_weights = F.softmax(attn_weights, dim=-1)\n",
    "        attn_weights = self.attn_dropout(attn_weights)\n",
    "\n",
    "        # Apply attention to values\n",
    "        attn_output = torch.matmul(attn_weights, v)\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "        attn_output = attn_output.reshape(batch_size, seq_len, self.hidden_size)\n",
    "\n",
    "        attn_output = self.out_proj(attn_output)\n",
    "        attn_output = self.resid_dropout(attn_output)\n",
    "\n",
    "        return attn_output\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    Position-wise feed-forward network with GELU activation.\n",
    "\n",
    "    Args:\n",
    "        hidden_size (int): Input/output dimension\n",
    "        intermediate_size (int): Hidden dimension\n",
    "        dropout (float): Dropout probability\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_size: int, intermediate_size: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(hidden_size, intermediate_size)\n",
    "        self.fc2 = nn.Linear(intermediate_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.fc1(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Single transformer encoder block with bidirectional attention.\n",
    "\n",
    "    Args:\n",
    "        hidden_size (int): Dimension of embeddings\n",
    "        num_attention_heads (int): Number of attention heads\n",
    "        intermediate_size (int): FFN intermediate dimension\n",
    "        dropout (float): Dropout probability\n",
    "        layer_norm_eps (float): Layer norm epsilon\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size: int,\n",
    "        num_attention_heads: int,\n",
    "        intermediate_size: int,\n",
    "        dropout: float = 0.1,\n",
    "        layer_norm_eps: float = 1e-5\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(hidden_size, eps=layer_norm_eps)\n",
    "        self.attn = MultiHeadAttention(hidden_size, num_attention_heads, dropout)\n",
    "        self.ln_2 = nn.LayerNorm(hidden_size, eps=layer_norm_eps)\n",
    "        self.ffn = FeedForward(hidden_size, intermediate_size, dropout)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Apply attention and FFN with residual connections.\"\"\"\n",
    "        # Attention block\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.ln_1(hidden_states)\n",
    "        hidden_states = self.attn(hidden_states, attention_mask)\n",
    "        hidden_states = residual + hidden_states\n",
    "\n",
    "        # FFN block\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.ln_2(hidden_states)\n",
    "        hidden_states = self.ffn(hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class BERTEmbeddingModel(nn.Module):\n",
    "    \"\"\"\n",
    "    BERT-style Bidirectional Encoder for generating text embeddings.\n",
    "\n",
    "    This model can be trained with:\n",
    "        1. Masked Language Modeling (MLM) - BERT-style pretraining\n",
    "        2. Contrastive Learning - SimCSE-style sentence embeddings\n",
    "        3. Both objectives simultaneously\n",
    "\n",
    "    The model generates dense vector representations of text that capture\n",
    "    semantic meaning and can be used for similarity search, clustering, etc.\n",
    "\n",
    "    Args:\n",
    "        vocab_size (int): Vocabulary size\n",
    "        hidden_size (int): Embedding dimension\n",
    "        num_hidden_layers (int): Number of transformer blocks\n",
    "        num_attention_heads (int): Number of attention heads\n",
    "        intermediate_size (int): FFN intermediate dimension\n",
    "        max_position_embeddings (int): Maximum sequence length\n",
    "        dropout (float): Dropout probability\n",
    "        layer_norm_eps (float): Layer norm epsilon\n",
    "        pooling_mode (str): Pooling strategy - 'mean', 'cls', or 'max'\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        hidden_size: int,\n",
    "        num_hidden_layers: int,\n",
    "        num_attention_heads: int,\n",
    "        intermediate_size: int,\n",
    "        max_position_embeddings: int,\n",
    "        dropout: float = 0.1,\n",
    "        layer_norm_eps: float = 1e-5,\n",
    "        pooling_mode: str = \"mean\"\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.pooling_mode = pooling_mode\n",
    "\n",
    "        # Embeddings\n",
    "        self.token_embeddings = nn.Embedding(vocab_size, hidden_size, padding_idx=0)\n",
    "        self.position_embeddings = nn.Embedding(max_position_embeddings, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Transformer encoder blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(\n",
    "                hidden_size,\n",
    "                num_attention_heads,\n",
    "                intermediate_size,\n",
    "                dropout,\n",
    "                layer_norm_eps\n",
    "            ) for _ in range(num_hidden_layers)\n",
    "        ])\n",
    "\n",
    "        self.ln_f = nn.LayerNorm(hidden_size, eps=layer_norm_eps)\n",
    "\n",
    "        # MLM head for masked language modeling\n",
    "        self.mlm_head = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.LayerNorm(hidden_size, eps=layer_norm_eps),\n",
    "            nn.Linear(hidden_size, vocab_size)\n",
    "        )\n",
    "\n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"Initialize weights with small random values.\"\"\"\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            torch.nn.init.zeros_(module.bias)\n",
    "            torch.nn.init.ones_(module.weight)\n",
    "\n",
    "    def get_attention_mask(self, attention_mask: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Convert attention mask to proper format for attention mechanism.\n",
    "\n",
    "        Args:\n",
    "            attention_mask: Binary mask (batch_size, seq_len)\n",
    "                           1 for real tokens, 0 for padding\n",
    "\n",
    "        Returns:\n",
    "            Extended mask (batch_size, 1, 1, seq_len)\n",
    "        \"\"\"\n",
    "        # Extend dimensions for broadcasting\n",
    "        extended_mask = attention_mask.unsqueeze(1).unsqueeze(2) # what is this? it is adding two dimensions to the attention mask\n",
    "        return extended_mask\n",
    "\n",
    "    def mean_pooling(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Mean pooling over sequence (excluding padding tokens).\n",
    "\n",
    "        This averages the token representations, weighted by the attention mask\n",
    "        to exclude padding tokens from the mean calculation.\n",
    "\n",
    "        Args:\n",
    "            hidden_states: Token representations (batch_size, seq_len, hidden_size)\n",
    "            attention_mask: Binary mask (batch_size, seq_len)\n",
    "\n",
    "        Returns:\n",
    "            Pooled embeddings (batch_size, hidden_size)\n",
    "        \"\"\"\n",
    "        # Expand mask to match hidden_states dimensions\n",
    "        mask_expanded = attention_mask.unsqueeze(-1).expand(hidden_states.size()).float()\n",
    "\n",
    "        # Sum of masked embeddings\n",
    "        sum_embeddings = torch.sum(hidden_states * mask_expanded, dim=1)\n",
    "\n",
    "        # Sum of mask (number of real tokens)\n",
    "        sum_mask = torch.clamp(mask_expanded.sum(dim=1), min=1e-9)\n",
    "\n",
    "        return sum_embeddings / sum_mask\n",
    "\n",
    "    def cls_pooling(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        CLS token pooling (use first token representation).\n",
    "\n",
    "        Args:\n",
    "            hidden_states: Token representations (batch_size, seq_len, hidden_size)\n",
    "\n",
    "        Returns:\n",
    "            CLS embeddings (batch_size, hidden_size)\n",
    "        \"\"\"\n",
    "        return hidden_states[:, 0, :]\n",
    "\n",
    "    def max_pooling(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Max pooling over sequence (excluding padding).\n",
    "\n",
    "        Args:\n",
    "            hidden_states: Token representations (batch_size, seq_len, hidden_size)\n",
    "            attention_mask: Binary mask (batch_size, seq_len)\n",
    "\n",
    "        Returns:\n",
    "            Max pooled embeddings (batch_size, hidden_size)\n",
    "        \"\"\"\n",
    "        # Set padding positions to very negative value before max\n",
    "        mask_expanded = attention_mask.unsqueeze(-1).expand(hidden_states.size()).float()\n",
    "        hidden_states = hidden_states.clone()\n",
    "        hidden_states[mask_expanded == 0] = -1e9\n",
    "\n",
    "        return torch.max(hidden_states, dim=1)[0]\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        masked_labels: Optional[torch.Tensor] = None,\n",
    "        return_embeddings: bool = True\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass of the embedding model.\n",
    "\n",
    "        Args:\n",
    "            input_ids: Token indices (batch_size, seq_len)\n",
    "            attention_mask: Binary mask (batch_size, seq_len), 1=real, 0=padding\n",
    "            masked_labels: Labels for MLM task (batch_size, seq_len), -100=ignore\n",
    "            return_embeddings: Whether to return pooled embeddings\n",
    "\n",
    "        Returns:\n",
    "            Dictionary containing:\n",
    "                - embeddings: Pooled sentence embeddings (if return_embeddings=True)\n",
    "                - mlm_logits: Token prediction logits (if masked_labels provided)\n",
    "                - mlm_loss: MLM loss (if masked_labels provided)\n",
    "                - hidden_states: Token-level representations\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = input_ids.size()\n",
    "        device = input_ids.device\n",
    "\n",
    "        # Create attention mask if not provided\n",
    "        if attention_mask is None:\n",
    "            attention_mask = (input_ids != 0).long()\n",
    "\n",
    "        # Position ids\n",
    "        position_ids = torch.arange(0, seq_len, dtype=torch.long, device=device)\n",
    "        position_ids = position_ids.unsqueeze(0).expand(batch_size, seq_len)\n",
    "\n",
    "        # Get embeddings\n",
    "        token_embeds = self.token_embeddings(input_ids)\n",
    "        position_embeds = self.position_embeddings(position_ids)\n",
    "        hidden_states = token_embeds + position_embeds\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "\n",
    "        # Prepare attention mask for transformer\n",
    "        extended_attention_mask = self.get_attention_mask(attention_mask)\n",
    "\n",
    "        # Apply transformer blocks\n",
    "        for block in self.blocks:\n",
    "            hidden_states = block(hidden_states, extended_attention_mask)\n",
    "\n",
    "        hidden_states = self.ln_f(hidden_states)\n",
    "\n",
    "        # Prepare output dictionary\n",
    "        outputs = {\"hidden_states\": hidden_states}\n",
    "\n",
    "        # Compute pooled embeddings\n",
    "        if return_embeddings:\n",
    "            if self.pooling_mode == \"mean\":\n",
    "                embeddings = self.mean_pooling(hidden_states, attention_mask)\n",
    "            elif self.pooling_mode == \"cls\":\n",
    "                embeddings = self.cls_pooling(hidden_states)\n",
    "            elif self.pooling_mode == \"max\":\n",
    "                embeddings = self.max_pooling(hidden_states, attention_mask)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown pooling mode: {self.pooling_mode}\")\n",
    "\n",
    "            # Normalize embeddings for better similarity computation\n",
    "            embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "            outputs[\"embeddings\"] = embeddings\n",
    "\n",
    "        # Compute MLM loss if labels provided\n",
    "        if masked_labels is not None:\n",
    "            mlm_logits = self.mlm_head(hidden_states)\n",
    "            mlm_loss = F.cross_entropy(\n",
    "                mlm_logits.view(-1, self.vocab_size),\n",
    "                masked_labels.view(-1),\n",
    "                ignore_index=-100\n",
    "            )\n",
    "            outputs[\"mlm_logits\"] = mlm_logits\n",
    "            outputs[\"mlm_loss\"] = mlm_loss\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "def compute_contrastive_loss(\n",
    "    embeddings: torch.Tensor,\n",
    "    temperature: float = 0.05\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute InfoNCE contrastive loss for embedding learning.\n",
    "\n",
    "    This implements a SimCSE-style contrastive objective where each example\n",
    "    should be similar to itself (positive pair) and different from others\n",
    "    (negative pairs). Works with augmented pairs or dropout-based augmentation.\n",
    "\n",
    "    Args:\n",
    "        embeddings: Normalized embeddings (batch_size * 2, hidden_size)\n",
    "                   First half and second half are augmented pairs\n",
    "        temperature: Temperature parameter for scaling similarities\n",
    "\n",
    "    Returns:\n",
    "        Contrastive loss scalar\n",
    "    \"\"\"\n",
    "    batch_size = embeddings.size(0) // 2\n",
    "\n",
    "    # Split into two views\n",
    "    embeddings_a = embeddings[:batch_size]\n",
    "    embeddings_b = embeddings[batch_size:]\n",
    "\n",
    "    # Compute similarity matrix\n",
    "    # Shape: (batch_size, batch_size)\n",
    "    sim_aa = torch.matmul(embeddings_a, embeddings_a.t()) / temperature\n",
    "    sim_bb = torch.matmul(embeddings_b, embeddings_b.t()) / temperature\n",
    "    sim_ab = torch.matmul(embeddings_a, embeddings_b.t()) / temperature\n",
    "    sim_ba = torch.matmul(embeddings_b, embeddings_a.t()) / temperature\n",
    "\n",
    "    # Labels: each example's positive pair is at the same index\n",
    "    labels = torch.arange(batch_size, device=embeddings.device)\n",
    "\n",
    "    # Contrastive loss for both directions\n",
    "    # For each anchor in view A, the positive is the corresponding sample in view B\n",
    "    loss_a = F.cross_entropy(\n",
    "        torch.cat([sim_ab, sim_aa], dim=1),\n",
    "        labels\n",
    "    )\n",
    "\n",
    "    loss_b = F.cross_entropy(\n",
    "        torch.cat([sim_ba, sim_bb], dim=1),\n",
    "        labels\n",
    "    )\n",
    "\n",
    "    return (loss_a + loss_b) / 2\n",
    "\n"
   ],
   "id": "3cc32485c7a7d200"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
