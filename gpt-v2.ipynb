{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T12:37:29.631110Z",
     "start_time": "2025-10-26T12:37:24.194316Z"
    }
   },
   "source": [
    "\"\"\"\n",
    "GPT-style Causal Language Model Implementation\n",
    "\n",
    "This module implements a decoder-only transformer architecture similar to GPT\n",
    "for causal language modeling tasks. The model is designed for pretraining on\n",
    "large text corpora using next-token prediction.\n",
    "\n",
    "Architecture:\n",
    "    - Token + Position Embeddings\n",
    "    - N Transformer Decoder Blocks\n",
    "    - Layer Normalization\n",
    "    - Language Modeling Head\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Optional, Tuple"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head self-attention mechanism with causal masking.\n",
    "\n",
    "    This implementation splits the hidden dimension across multiple attention heads,\n",
    "    allowing the model to attend to different representation subspaces simultaneously.\n",
    "    Causal masking ensures that positions can only attend to earlier positions.\n",
    "\n",
    "    Args:\n",
    "        hidden_size (int): Dimension of input embeddings\n",
    "        num_attention_heads (int): Number of parallel attention heads\n",
    "        dropout (float): Dropout probability for attention weights\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_size: int, num_attention_heads: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        assert hidden_size % num_attention_heads == 0, \\\n",
    "            \"hidden_size must be divisible by num_attention_heads\"\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.head_dim = hidden_size // num_attention_heads\n",
    "        self.scale = math.sqrt(self.head_dim)\n",
    "\n",
    "        # Query, Key, Value projections for all heads (batched)\n",
    "        self.qkv_proj = nn.Linear(hidden_size, 3 * hidden_size, bias=True)\n",
    "        self.out_proj = nn.Linear(hidden_size, hidden_size, bias=True)\n",
    "\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.resid_dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of multi-head attention.\n",
    "\n",
    "        Args:\n",
    "            hidden_states: Input tensor of shape (batch_size, seq_len, hidden_size)\n",
    "            attention_mask: Optional mask of shape (batch_size, 1, 1, seq_len)\n",
    "\n",
    "        Returns:\n",
    "            Output tensor of shape (batch_size, seq_len, hidden_size)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = hidden_states.size()\n",
    "\n",
    "        # Project to Q, K, V and split into multiple heads\n",
    "        qkv = self.qkv_proj(hidden_states)\n",
    "        qkv = qkv.reshape(batch_size, seq_len, 3, self.num_attention_heads, self.head_dim)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)  # (3, batch, heads, seq_len, head_dim)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        # Compute attention scores: (batch, heads, seq_len, seq_len)\n",
    "        attn_weights = torch.matmul(q, k.transpose(-2, -1)) / self.scale\n",
    "\n",
    "        # Apply causal mask (prevent attending to future tokens)\n",
    "        if attention_mask is not None:\n",
    "            attn_weights = attn_weights.masked_fill(attention_mask == 0, float('-inf'))\n",
    "\n",
    "        # Softmax and dropout\n",
    "        attn_weights = F.softmax(attn_weights, dim=-1)\n",
    "        attn_weights = self.attn_dropout(attn_weights)\n",
    "\n",
    "        # Apply attention to values\n",
    "        attn_output = torch.matmul(attn_weights, v)  # (batch, heads, seq_len, head_dim)\n",
    "\n",
    "        # Concatenate heads and project back\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "        attn_output = attn_output.reshape(batch_size, seq_len, self.hidden_size)\n",
    "        attn_output = self.out_proj(attn_output)\n",
    "        attn_output = self.resid_dropout(attn_output)\n",
    "\n",
    "        return attn_output\n"
   ],
   "id": "1c75fd83e6a05c9f",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    Position-wise feed-forward network.\n",
    "\n",
    "    A two-layer MLP with GELU activation, applied independently to each position.\n",
    "    This adds non-linearity and increases model capacity.\n",
    "\n",
    "    Args:\n",
    "        hidden_size (int): Input/output dimension\n",
    "        intermediate_size (int): Hidden dimension (typically 4x hidden_size)\n",
    "        dropout (float): Dropout probability\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_size: int, intermediate_size: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(hidden_size, intermediate_size)\n",
    "        self.fc2 = nn.Linear(intermediate_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Apply two-layer FFN with GELU activation.\"\"\"\n",
    "        x = self.fc1(x)\n",
    "        x = F.gelu(x)  # Gaussian Error Linear Unit\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ],
   "id": "3c3494d2fac17a7",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Single transformer decoder block.\n",
    "\n",
    "    Consists of:\n",
    "        1. Multi-head self-attention with causal masking\n",
    "        2. Feed-forward network\n",
    "        3. Layer normalization and residual connections\n",
    "\n",
    "    Args:\n",
    "        hidden_size (int): Dimension of embeddings\n",
    "        num_attention_heads (int): Number of attention heads\n",
    "        intermediate_size (int): FFN intermediate dimension\n",
    "        dropout (float): Dropout probability\n",
    "        layer_norm_eps (float): Layer norm epsilon for numerical stability\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size: int,\n",
    "        num_attention_heads: int,\n",
    "        intermediate_size: int,\n",
    "        dropout: float = 0.1,\n",
    "        layer_norm_eps: float = 1e-5\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(hidden_size, eps=layer_norm_eps)\n",
    "        self.attn = MultiHeadAttention(hidden_size, num_attention_heads, dropout)\n",
    "        self.ln_2 = nn.LayerNorm(hidden_size, eps=layer_norm_eps)\n",
    "        self.ffn = FeedForward(hidden_size, intermediate_size, dropout)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass with pre-norm architecture.\n",
    "\n",
    "        Args:\n",
    "            hidden_states: Input tensor (batch_size, seq_len, hidden_size)\n",
    "            attention_mask: Optional attention mask\n",
    "\n",
    "        Returns:\n",
    "            Output tensor (batch_size, seq_len, hidden_size)\n",
    "        \"\"\"\n",
    "        # Self-attention with residual connection\n",
    "        residual = hidden_states # why residual connection? because we want to add the input to the output of the attention block\n",
    "        hidden_states = self.ln_1(hidden_states)\n",
    "        hidden_states = self.attn(hidden_states, attention_mask)\n",
    "        hidden_states = residual + hidden_states\n",
    "\n",
    "        # Feed-forward with residual connection\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.ln_2(hidden_states)\n",
    "        hidden_states = self.ffn(hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "\n",
    "        return hidden_states\n"
   ],
   "id": "d52aeb7efca693a3",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "class GPTLMModel(nn.Module):\n",
    "    \"\"\"\n",
    "    GPT-style Causal Language Model.\n",
    "\n",
    "    A decoder-only transformer architecture for autoregressive language modeling.\n",
    "    The model predicts the next token given all previous tokens in the sequence.\n",
    "\n",
    "    Architecture components:\n",
    "        - Token embeddings\n",
    "        - Learned positional embeddings\n",
    "        - Stack of transformer decoder blocks\n",
    "        - Final layer normalization\n",
    "        - Language modeling head (projects to vocabulary)\n",
    "\n",
    "    Args:\n",
    "        vocab_size (int): Size of vocabulary\n",
    "        hidden_size (int): Dimension of embeddings\n",
    "        num_hidden_layers (int): Number of transformer blocks\n",
    "        num_attention_heads (int): Number of attention heads per block\n",
    "        intermediate_size (int): FFN intermediate dimension\n",
    "        max_position_embeddings (int): Maximum sequence length\n",
    "        dropout (float): Dropout probability\n",
    "        layer_norm_eps (float): Layer norm epsilon\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        hidden_size: int,\n",
    "        num_hidden_layers: int,\n",
    "        num_attention_heads: int,\n",
    "        intermediate_size: int,\n",
    "        max_position_embeddings: int,\n",
    "        dropout: float = 0.1,\n",
    "        layer_norm_eps: float = 1e-5\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "\n",
    "        # Embeddings\n",
    "        self.token_embeddings = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.position_embeddings = nn.Embedding(max_position_embeddings, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Transformer blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(\n",
    "                hidden_size,\n",
    "                num_attention_heads,\n",
    "                intermediate_size,\n",
    "                dropout,\n",
    "                layer_norm_eps\n",
    "            ) for _ in range(num_hidden_layers)\n",
    "        ])\n",
    "\n",
    "        # Final layer norm\n",
    "        self.ln_f = nn.LayerNorm(hidden_size, eps=layer_norm_eps)\n",
    "\n",
    "        # Language modeling head\n",
    "        self.lm_head = nn.Linear(hidden_size, vocab_size, bias=False)\n",
    "\n",
    "        # Weight tying: share weights between token embeddings and lm_head\n",
    "        self.lm_head.weight = self.token_embeddings.weight\n",
    "\n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"Initialize model weights using Xavier/He initialization.\"\"\"\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            torch.nn.init.zeros_(module.bias)\n",
    "            torch.nn.init.ones_(module.weight)\n",
    "\n",
    "    def get_causal_mask(self, seq_len: int, device: torch.device) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Create causal attention mask to prevent attending to future tokens.\n",
    "\n",
    "        Args:\n",
    "            seq_len: Length of sequence\n",
    "            device: Device to create mask on\n",
    "\n",
    "        Returns:\n",
    "            Causal mask of shape (1, 1, seq_len, seq_len)\n",
    "        \"\"\"\n",
    "        mask = torch.tril(torch.ones(seq_len, seq_len, device=device))\n",
    "        mask = mask.unsqueeze(0).unsqueeze(0)  # Add batch and head dimensions\n",
    "        return mask\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        labels: Optional[torch.Tensor] = None\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "\n",
    "        Args:\n",
    "            input_ids: Token indices of shape (batch_size, seq_len)\n",
    "            labels: Optional labels for computing loss (batch_size, seq_len)\n",
    "\n",
    "        Returns:\n",
    "            logits: Token prediction logits (batch_size, seq_len, vocab_size)\n",
    "            loss: Cross-entropy loss if labels provided, else None\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = input_ids.size()\n",
    "        device = input_ids.device\n",
    "\n",
    "        # Create position ids\n",
    "        position_ids = torch.arange(0, seq_len, dtype=torch.long, device=device)\n",
    "        position_ids = position_ids.unsqueeze(0).expand(batch_size, seq_len)\n",
    "\n",
    "        # Get embeddings\n",
    "        token_embeds = self.token_embeddings(input_ids)\n",
    "        position_embeds = self.position_embeddings(position_ids)\n",
    "        hidden_states = token_embeds + position_embeds\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "\n",
    "        # Create causal mask\n",
    "        attention_mask = self.get_causal_mask(seq_len, device)\n",
    "\n",
    "        # Apply transformer blocks\n",
    "        for block in self.blocks:\n",
    "            hidden_states = block(hidden_states, attention_mask)\n",
    "\n",
    "        # Final layer norm\n",
    "        hidden_states = self.ln_f(hidden_states)\n",
    "\n",
    "        # Get logits\n",
    "        logits = self.lm_head(hidden_states)\n",
    "\n",
    "        # Compute loss if labels provided\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # Shift logits and labels for next-token prediction\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "\n",
    "            # Flatten and compute cross-entropy loss\n",
    "            loss = F.cross_entropy(\n",
    "                shift_logits.view(-1, self.vocab_size),\n",
    "                shift_labels.view(-1),\n",
    "                ignore_index=-100  # Ignore padding tokens\n",
    "            )\n",
    "\n",
    "        return logits, loss\n",
    "\n"
   ],
   "id": "bfa7ef559340e47"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
