{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T04:46:35.842846Z",
     "start_time": "2025-10-27T04:46:16.731827Z"
    }
   },
   "source": [
    "\"\"\"\n",
    "LLM Pretraining Script\n",
    "======================\n",
    "Modern implementation for pretraining language models from scratch using the latest techniques:\n",
    "- Flash Attention 2 for efficient attention computation\n",
    "- Mixed precision training (bfloat16)\n",
    "- Gradient checkpointing for memory efficiency\n",
    "- AdamW optimizer with cosine learning rate schedule\n",
    "- Gradient accumulation for large effective batch sizes\n",
    "- Distributed Data Parallel (DDP) support\n",
    "\n",
    "Dataset: BEE-spoke-data/TxT360-1M-sample (1M high-quality text samples)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.distributed as dist\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Dict, Any\n",
    "import wandb\n",
    "from tqdm.auto import tqdm\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T04:47:48.507591Z",
     "start_time": "2025-10-27T04:47:48.488416Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    \"\"\"Configuration for the LLM architecture.\"\"\"\n",
    "\n",
    "    vocab_size: int = 50304  # Padded to nearest multiple of 64 for efficiency\n",
    "    max_seq_length: int = 2048\n",
    "    hidden_size: int = 2048\n",
    "    num_hidden_layers: int = 24\n",
    "    num_attention_heads: int = 16\n",
    "    num_key_value_heads: int = 8  # Grouped-Query Attention (GQA)\n",
    "    intermediate_size: int = 5632  # ~2.7x hidden_size (SwiGLU)\n",
    "    hidden_act: str = \"silu\"\n",
    "    rms_norm_eps: float = 1e-6\n",
    "    rope_theta: float = 10000.0\n",
    "    attention_dropout: float = 0.0\n",
    "    hidden_dropout: float = 0.0\n",
    "    use_flash_attention: bool = True\n",
    "    gradient_checkpointing: bool = True"
   ],
   "id": "ccc6284898e86c1a",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T04:47:54.960273Z",
     "start_time": "2025-10-27T04:47:54.940716Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    \"\"\"Configuration for training hyperparameters.\"\"\"\n",
    "\n",
    "    # Data\n",
    "    dataset_name: str = \"BEE-spoke-data/TxT360-1M-sample\"\n",
    "    dataset_split: str = \"train\"\n",
    "    max_seq_length: int = 2048\n",
    "\n",
    "    # Training\n",
    "    batch_size: int = 4\n",
    "    gradient_accumulation_steps: int = 32  # Effective batch size = 128\n",
    "    num_epochs: int = 1\n",
    "    max_steps: Optional[int] = None\n",
    "\n",
    "    # Optimization\n",
    "    learning_rate: float = 3e-4\n",
    "    weight_decay: float = 0.1\n",
    "    adam_beta1: float = 0.9\n",
    "    adam_beta2: float = 0.95\n",
    "    adam_epsilon: float = 1e-8\n",
    "    max_grad_norm: float = 1.0\n",
    "    warmup_steps: int = 2000\n",
    "\n",
    "    # Mixed precision\n",
    "    use_fp16: bool = False\n",
    "    use_bf16: bool = True\n",
    "\n",
    "    # Checkpointing\n",
    "    save_steps: int = 5000\n",
    "    save_total_limit: int = 3\n",
    "    output_dir: str = \"./checkpoints/llm\"\n",
    "\n",
    "    # Logging\n",
    "    logging_steps: int = 10\n",
    "    use_wandb: bool = False\n",
    "    wandb_project: str = \"llm-pretraining\"\n",
    "\n",
    "    # Distributed\n",
    "    local_rank: int = -1\n",
    "    world_size: int = 1"
   ],
   "id": "fb1b58b611d0831b",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T04:53:00.222177Z",
     "start_time": "2025-10-27T04:53:00.213434Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    \"\"\"Root Mean Square Layer Normalization (more efficient than LayerNorm).\"\"\"\n",
    "\n",
    "    def __init__(self, hidden_size: int, eps: float = 1e-6):\n",
    "        \"\"\"\n",
    "        Initialize RMSNorm layer.\n",
    "\n",
    "        Args:\n",
    "            hidden_size: Dimension of the hidden states\n",
    "            eps: Epsilon for numerical stability\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(hidden_size))\n",
    "        self.variance_epsilon = eps\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Apply RMS normalization.\n",
    "\n",
    "        Args:\n",
    "            hidden_states: Input tensor of shape (batch, seq_len, hidden_size)\n",
    "\n",
    "        Returns:\n",
    "            Normalized tensor of same shape\n",
    "        \"\"\"\n",
    "        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
    "        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
    "        return self.weight * hidden_states\n"
   ],
   "id": "eb8e5a2c229dfe0c",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T04:57:13.779191Z",
     "start_time": "2025-10-27T04:57:13.768161Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "class RotaryPositionalEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Rotary Position Embedding (RoPE) for better length extrapolation.\n",
    "    Implementation follows the original paper: https://arxiv.org/abs/2104.09864\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim: int, max_seq_length: int = 2048, base: float = 10000.0):\n",
    "        \"\"\"\n",
    "        Initialize RoPE.\n",
    "\n",
    "        Args:\n",
    "            dim: Dimension of each attention head\n",
    "            max_seq_length: Maximum sequence length\n",
    "            base: Base for the geometric progression\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.base = base\n",
    "\n",
    "        # Precompute frequencies\n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
    "\n",
    "        # Build position embeddings cache\n",
    "        self._set_cos_sin_cache(max_seq_length)\n",
    "\n",
    "    def _set_cos_sin_cache(self, seq_len: int):\n",
    "        \"\"\"Precompute cos and sin values for efficiency.\"\"\"\n",
    "        self.max_seq_len_cached = seq_len\n",
    "        t = torch.arange(seq_len, device=self.inv_freq.device).type_as(self.inv_freq)\n",
    "        freqs = torch.outer(t, self.inv_freq)\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        self.register_buffer(\"cos_cached\", emb.cos(), persistent=False)\n",
    "        self.register_buffer(\"sin_cached\", emb.sin(), persistent=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, seq_len: int):\n",
    "        \"\"\"\n",
    "        Apply rotary embeddings.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor\n",
    "            seq_len: Sequence length\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (cos, sin) tensors for rotary embedding\n",
    "        \"\"\"\n",
    "        if seq_len > self.max_seq_len_cached:\n",
    "            self._set_cos_sin_cache(seq_len)\n",
    "\n",
    "        return (\n",
    "            self.cos_cached[:seq_len],\n",
    "            self.sin_cached[:seq_len],\n",
    "        )\n"
   ],
   "id": "faba62792f13adb6",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T05:11:12.486198Z",
     "start_time": "2025-10-27T05:11:12.475505Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def rotate_half(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Helper function to rotate half the hidden dims of the input.\"\"\"\n",
    "    x1, x2 = x.chunk(2, dim=-1)\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "\n",
    "def apply_rotary_pos_emb(q: torch.Tensor, k: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Apply rotary position embeddings to queries and keys.\n",
    "\n",
    "    Args:\n",
    "        q: Query tensor\n",
    "        k: Key tensor\n",
    "        cos: Cosine tensor\n",
    "        sin: Sine tensor\n",
    "\n",
    "    Returns:\n",
    "        Tuple of rotated (q, k) tensors\n",
    "    \"\"\"\n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "    return q_embed, k_embed\n"
   ],
   "id": "1b91fb44e345bbc3",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "class GroupedQueryAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Grouped-Query Attention (GQA) - hybrid between Multi-Head and Multi-Query Attention.\n",
    "    More efficient than MHA while maintaining better quality than MQA.\n",
    "    Paper: https://arxiv.org/abs/2305.13245\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        \"\"\"\n",
    "        Initialize GQA layer.\n",
    "\n",
    "        Args:\n",
    "            config: Model configuration\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.num_heads = config.num_attention_heads\n",
    "        self.num_key_value_heads = config.num_key_value_heads\n",
    "        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n",
    "        self.head_dim = self.hidden_size // self.num_heads\n",
    "        self.attention_dropout = config.attention_dropout\n",
    "        self.use_flash_attention = config.use_flash_attention\n",
    "\n",
    "        # Projections\n",
    "        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=False)\n",
    "        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)\n",
    "        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)\n",
    "        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)\n",
    "\n",
    "        # RoPE\n",
    "        self.rotary_emb = RotaryPositionalEmbedding(\n",
    "            self.head_dim,\n",
    "            max_seq_length=config.max_seq_length,\n",
    "            base=config.rope_theta,\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of GQA.\n",
    "\n",
    "        Args:\n",
    "            hidden_states: Input tensor of shape (batch, seq_len, hidden_size)\n",
    "            attention_mask: Optional attention mask\n",
    "\n",
    "        Returns:\n",
    "            Output tensor of shape (batch, seq_len, hidden_size)\n",
    "        \"\"\"\n",
    "        batch_size, seq_length, _ = hidden_states.shape\n",
    "\n",
    "        # Project to Q, K, V\n",
    "        query_states = self.q_proj(hidden_states)\n",
    "        key_states = self.k_proj(hidden_states)\n",
    "        value_states = self.v_proj(hidden_states)\n",
    "\n",
    "        # Reshape for multi-head attention\n",
    "        query_states = query_states.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        key_states = key_states.view(batch_size, seq_length, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
    "        value_states = value_states.view(batch_size, seq_length, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # Apply RoPE\n",
    "        cos, sin = self.rotary_emb(value_states, seq_length)\n",
    "        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n",
    "\n",
    "        # Repeat K, V for grouped-query attention\n",
    "        if self.num_key_value_groups > 1:\n",
    "            key_states = key_states.repeat_interleave(self.num_key_value_groups, dim=1)\n",
    "            value_states = value_states.repeat_interleave(self.num_key_value_groups, dim=1)\n",
    "\n",
    "        # Flash Attention or standard attention\n",
    "        if self.use_flash_attention and hasattr(F, 'scaled_dot_product_attention'):\n",
    "            # Use PyTorch's native flash attention (requires PyTorch 2.0+)\n",
    "            attn_output = F.scaled_dot_product_attention(\n",
    "                query_states,\n",
    "                key_states,\n",
    "                value_states,\n",
    "                attn_mask=attention_mask,\n",
    "                dropout_p=self.attention_dropout if self.training else 0.0,\n",
    "                is_causal=True,\n",
    "            )\n",
    "        else:\n",
    "            # Standard attention computation\n",
    "            attn_weights = torch.matmul(query_states, key_states.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "\n",
    "            if attention_mask is not None:\n",
    "                attn_weights = attn_weights + attention_mask\n",
    "\n",
    "            attn_weights = F.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
    "            attn_weights = F.dropout(attn_weights, p=self.attention_dropout, training=self.training)\n",
    "            attn_output = torch.matmul(attn_weights, value_states)\n",
    "\n",
    "        # Reshape and project output\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "        attn_output = attn_output.reshape(batch_size, seq_length, self.hidden_size)\n",
    "        attn_output = self.o_proj(attn_output)\n",
    "\n",
    "        return attn_output\n"
   ],
   "id": "c41ab069a881eb35",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "class SwiGLUMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    SwiGLU MLP - Gated Linear Unit with Swish activation.\n",
    "    More efficient and better performing than standard FFN.\n",
    "    Paper: https://arxiv.org/abs/2002.05202\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        \"\"\"\n",
    "        Initialize SwiGLU MLP.\n",
    "\n",
    "        Args:\n",
    "            config: Model configuration\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.intermediate_size = config.intermediate_size\n",
    "\n",
    "        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n",
    "        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n",
    "        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n",
    "        self.act_fn = nn.SiLU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of SwiGLU MLP.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor of shape (batch, seq_len, hidden_size)\n",
    "\n",
    "        Returns:\n",
    "            Output tensor of shape (batch, seq_len, hidden_size)\n",
    "        \"\"\"\n",
    "        return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n"
   ],
   "id": "74577d1eea9da477",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"Single transformer block with pre-normalization.\"\"\"\n",
    "\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        \"\"\"\n",
    "        Initialize transformer block.\n",
    "\n",
    "        Args:\n",
    "            config: Model configuration\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.attention = GroupedQueryAttention(config)\n",
    "        self.mlp = SwiGLUMLP(config)\n",
    "        self.input_layernorm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        self.post_attention_layernorm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass with residual connections.\n",
    "\n",
    "        Args:\n",
    "            hidden_states: Input tensor\n",
    "            attention_mask: Optional attention mask\n",
    "\n",
    "        Returns:\n",
    "            Output tensor\n",
    "        \"\"\"\n",
    "        # Self-attention with pre-norm\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.input_layernorm(hidden_states)\n",
    "        hidden_states = self.attention(hidden_states, attention_mask)\n",
    "        hidden_states = residual + hidden_states\n",
    "\n",
    "        # MLP with pre-norm\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states)\n",
    "        hidden_states = self.mlp(hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "\n",
    "        return hidden_states\n"
   ],
   "id": "3bd3df26bdd15d84",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "class LLMModel(nn.Module):\n",
    "    \"\"\"Complete Language Model architecture.\"\"\"\n",
    "\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        \"\"\"\n",
    "        Initialize LLM.\n",
    "\n",
    "        Args:\n",
    "            config: Model configuration\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size) # embed_tokens is the embedding layer for the input tokens\n",
    "        self.layers = nn.ModuleList([TransformerBlock(config) for _ in range(config.num_hidden_layers)]) # layers is the list of transformer blocks\n",
    "        self.norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps) # norm is the layer normalization layer\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False) # lm_head is the linear layer for the output logits\n",
    "\n",
    "        # Tie embeddings\n",
    "        self.lm_head.weight = self.embed_tokens.weight # tie the weights of the linear layer to the weights of the embedding layer\n",
    "\n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"Initialize weights using scaled initialization.\"\"\"\n",
    "        std = 0.02\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=std)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=std)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        labels: Optional[torch.Tensor] = None,\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "\n",
    "        Args:\n",
    "            input_ids: Token IDs of shape (batch, seq_len)\n",
    "            attention_mask: Optional attention mask\n",
    "            labels: Optional labels for computing loss\n",
    "\n",
    "        Returns:\n",
    "            Dictionary containing loss and logits\n",
    "        \"\"\"\n",
    "        # Embed tokens\n",
    "        hidden_states = self.embed_tokens(input_ids)\n",
    "\n",
    "        # Prepare causal attention mask\n",
    "        batch_size, seq_length = input_ids.shape\n",
    "        causal_mask = torch.triu(\n",
    "            torch.full((seq_length, seq_length), float(\"-inf\"), device=input_ids.device),\n",
    "            diagonal=1,\n",
    "        )\n",
    "\n",
    "        # Pass through transformer blocks\n",
    "        for layer in self.layers:\n",
    "            hidden_states = layer(hidden_states, causal_mask)\n",
    "\n",
    "        # Final normalization and projection\n",
    "        hidden_states = self.norm(hidden_states)\n",
    "        logits = self.lm_head(hidden_states)\n",
    "\n",
    "        # Compute loss if labels provided\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # Shift logits and labels for next-token prediction\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "\n",
    "            # Compute cross-entropy loss\n",
    "            loss = F.cross_entropy(\n",
    "                shift_logits.view(-1, shift_logits.size(-1)),\n",
    "                shift_labels.view(-1),\n",
    "                ignore_index=-100,\n",
    "            )\n",
    "\n",
    "        return {\"loss\": loss, \"logits\": logits}\n"
   ],
   "id": "aa40b9f046c617e9",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "class TextDataset(Dataset):\n",
    "    \"\"\"Dataset for loading and tokenizing text data.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset_name: str,\n",
    "        tokenizer: PreTrainedTokenizerFast,\n",
    "        max_length: int = 2048,\n",
    "        split: str = \"train\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize dataset.\n",
    "\n",
    "        Args:\n",
    "            dataset_name: Name of the HuggingFace dataset\n",
    "            tokenizer: Tokenizer instance\n",
    "            max_length: Maximum sequence length\n",
    "            split: Dataset split to use\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "        logger.info(f\"Loading dataset {dataset_name}...\")\n",
    "        self.dataset = load_dataset(dataset_name, split=split, streaming=False)\n",
    "        logger.info(f\"Dataset loaded with {len(self.dataset)} samples\")\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Get a single tokenized sample.\n",
    "\n",
    "        Args:\n",
    "            idx: Sample index\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with input_ids and attention_mask\n",
    "        \"\"\"\n",
    "        text = self.dataset[idx][\"text\"]\n",
    "\n",
    "        # Tokenize\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        # Prepare input_ids and labels (for causal LM, labels = input_ids)\n",
    "        input_ids = encoding[\"input_ids\"].squeeze(0)\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"labels\": input_ids.clone(),\n",
    "        }"
   ],
   "id": "92905ec4b4fc2c36",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "\n",
    "def train_tokenizer(dataset_name: str, vocab_size: int = 50304, output_path: str = \"./tokenizer\") -> PreTrainedTokenizerFast:\n",
    "    \"\"\"\n",
    "    Train a BPE tokenizer on the dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset_name: Name of the HuggingFace dataset\n",
    "        vocab_size: Vocabulary size\n",
    "        output_path: Path to save tokenizer\n",
    "\n",
    "    Returns:\n",
    "        Trained tokenizer\n",
    "    \"\"\"\n",
    "    logger.info(\"Training tokenizer...\")\n",
    "\n",
    "    # Load dataset\n",
    "    dataset = load_dataset(dataset_name, split=\"train\", streaming=True)\n",
    "\n",
    "    # Create iterator for training\n",
    "    def batch_iterator(batch_size=1000):\n",
    "        for i, batch in enumerate(dataset):\n",
    "            if i >= 100000:  # Use first 100k samples for tokenizer training\n",
    "                break\n",
    "            yield batch[\"text\"]\n",
    "\n",
    "    # Initialize tokenizer\n",
    "    tokenizer = Tokenizer(BPE(unk_token=\"<unk>\"))\n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "    # Train tokenizer\n",
    "    trainer = BpeTrainer(\n",
    "        vocab_size=vocab_size,\n",
    "        special_tokens=[\"<unk>\", \"<s>\", \"</s>\", \"<pad>\"],\n",
    "        show_progress=True,\n",
    "    )\n",
    "\n",
    "    tokenizer.train_from_iterator(batch_iterator(), trainer=trainer)\n",
    "\n",
    "    # Add post-processor\n",
    "    tokenizer.post_processor = TemplateProcessing(\n",
    "        single=\"<s> $A </s>\",\n",
    "        special_tokens=[(\"<s>\", 1), (\"</s>\", 2)],\n",
    "    )\n",
    "\n",
    "    # Convert to HuggingFace tokenizer\n",
    "    hf_tokenizer = PreTrainedTokenizerFast(\n",
    "        tokenizer_object=tokenizer,\n",
    "        unk_token=\"<unk>\",\n",
    "        bos_token=\"<s>\",\n",
    "        eos_token=\"</s>\",\n",
    "        pad_token=\"<pad>\",\n",
    "    )\n",
    "\n",
    "    # Save tokenizer\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    hf_tokenizer.save_pretrained(output_path)\n",
    "    logger.info(f\"Tokenizer saved to {output_path}\")\n",
    "\n",
    "    return hf_tokenizer\n"
   ],
   "id": "a1ba29eb0e0bc1dc",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "def setup_distributed():\n",
    "    \"\"\"Initialize distributed training.\"\"\"\n",
    "    if \"RANK\" in os.environ and \"WORLD_SIZE\" in os.environ:\n",
    "        rank = int(os.environ[\"RANK\"]) # rank is the process id\n",
    "        world_size = int(os.environ[\"WORLD_SIZE\"]) # world_size is the total number of processes\n",
    "        local_rank = int(os.environ[\"LOCAL_RANK\"]) # local_rank is the rank of the process on the current node\n",
    "\n",
    "        dist.init_process_group(\"nccl\")\n",
    "        torch.cuda.set_device(local_rank) # set the device to the local rank\n",
    "\n",
    "        return rank, world_size, local_rank\n",
    "    return 0, 1, -1"
   ],
   "id": "e679b9143a1bbf3b",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "\n",
    "def train(config: TrainingConfig):\n",
    "    \"\"\"\n",
    "    Main training function.\n",
    "\n",
    "    Args:\n",
    "        config: Training configuration\n",
    "    \"\"\"\n",
    "    # Setup distributed training\n",
    "    rank, world_size, local_rank = setup_distributed()\n",
    "    is_main_process = rank == 0\n",
    "\n",
    "    if is_main_process:\n",
    "        logger.info(\"Starting LLM pretraining...\")\n",
    "        if config.use_wandb:\n",
    "            wandb.init(project=config.wandb_project, config=config.__dict__)\n",
    "\n",
    "    # Set device\n",
    "    device = torch.device(f\"cuda:{local_rank}\" if local_rank >= 0 else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Train or load tokenizer\n",
    "    tokenizer_path = \"./tokenizer\"\n",
    "    if is_main_process and not os.path.exists(tokenizer_path):\n",
    "        tokenizer = train_tokenizer(config.dataset_name, output_path=tokenizer_path)\n",
    "\n",
    "    if world_size > 1:\n",
    "        dist.barrier()  # Wait for main process to finish tokenizer training\n",
    "\n",
    "    tokenizer = PreTrainedTokenizerFast.from_pretrained(tokenizer_path)\n",
    "\n",
    "    # Create model\n",
    "    model_config = ModelConfig(vocab_size=len(tokenizer), max_seq_length=config.max_seq_length)\n",
    "    model = LLMModel(model_config).to(device)\n",
    "\n",
    "    if is_main_process:\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        logger.info(f\"Model parameters: {total_params:,} ({total_params / 1e9:.2f}B)\")\n",
    "\n",
    "    # Enable gradient checkpointing\n",
    "    if model_config.gradient_checkpointing:\n",
    "        model.gradient_checkpointing_enable()\n",
    "\n",
    "    # Wrap model with DDP\n",
    "    if world_size > 1:\n",
    "        model = DDP(model, device_ids=[local_rank], output_device=local_rank)\n",
    "\n",
    "    # Create dataset and dataloader\n",
    "    dataset = TextDataset(config.dataset_name, tokenizer, config.max_seq_length, config.dataset_split)\n",
    "    sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank) if world_size > 1 else None\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=config.batch_size,\n",
    "        sampler=sampler,\n",
    "        shuffle=(sampler is None),\n",
    "        num_workers=4,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    # Setup optimizer\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=config.learning_rate,\n",
    "        betas=(config.adam_beta1, config.adam_beta2),\n",
    "        eps=config.adam_epsilon,\n",
    "        weight_decay=config.weight_decay,\n",
    "    )\n",
    "\n",
    "    # Calculate total steps\n",
    "    total_steps = config.max_steps if config.max_steps else len(dataloader) * config.num_epochs // config.gradient_accumulation_steps\n",
    "\n",
    "    # Setup scheduler\n",
    "    scheduler = get_cosine_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=config.warmup_steps,\n",
    "        num_training_steps=total_steps,\n",
    "    )\n",
    "\n",
    "    # Setup mixed precision\n",
    "    scaler = torch.cuda.amp.GradScaler() if config.use_fp16 else None\n",
    "    dtype = torch.bfloat16 if config.use_bf16 else torch.float16 if config.use_fp16 else torch.float32\n",
    "\n",
    "    # Training loop\n",
    "    global_step = 0\n",
    "    model.zero_grad()\n",
    "\n",
    "    for epoch in range(config.num_epochs):\n",
    "        if sampler:\n",
    "            sampler.set_epoch(epoch)\n",
    "\n",
    "        progress_bar = tqdm(dataloader, disable=not is_main_process)\n",
    "\n",
    "        for step, batch in enumerate(progress_bar):\n",
    "            # Move batch to device\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            # Forward pass with mixed precision\n",
    "            with torch.cuda.amp.autocast(dtype=dtype):\n",
    "                outputs = model(input_ids=input_ids, labels=labels)\n",
    "                loss = outputs[\"loss\"] / config.gradient_accumulation_steps\n",
    "\n",
    "            # Backward pass\n",
    "            if scaler:\n",
    "                scaler.scale(loss).backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "\n",
    "            # Update weights\n",
    "            if (step + 1) % config.gradient_accumulation_steps == 0:\n",
    "                if scaler:\n",
    "                    scaler.unscale_(optimizer)\n",
    "\n",
    "                # Gradient clipping\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), config.max_grad_norm)\n",
    "\n",
    "                if scaler:\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                else:\n",
    "                    optimizer.step()\n",
    "\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                global_step += 1\n",
    "\n",
    "                # Logging\n",
    "                if global_step % config.logging_steps == 0 and is_main_process:\n",
    "                    lr = scheduler.get_last_lr()[0]\n",
    "                    loss_val = loss.item() * config.gradient_accumulation_steps\n",
    "                    progress_bar.set_postfix({\"loss\": f\"{loss_val:.4f}\", \"lr\": f\"{lr:.2e}\"})\n",
    "\n",
    "                    if config.use_wandb:\n",
    "                        wandb.log({\n",
    "                            \"train/loss\": loss_val,\n",
    "                            \"train/learning_rate\": lr,\n",
    "                            \"train/step\": global_step,\n",
    "                        })\n",
    "\n",
    "                # Save checkpoint\n",
    "                if global_step % config.save_steps == 0 and is_main_process:\n",
    "                    checkpoint_path = os.path.join(config.output_dir, f\"checkpoint-{global_step}\")\n",
    "                    os.makedirs(checkpoint_path, exist_ok=True)\n",
    "\n",
    "                    model_to_save = model.module if hasattr(model, \"module\") else model\n",
    "                    torch.save({\n",
    "                        \"model\": model_to_save.state_dict(),\n",
    "                        \"optimizer\": optimizer.state_dict(),\n",
    "                        \"scheduler\": scheduler.state_dict(),\n",
    "                        \"step\": global_step,\n",
    "                        \"config\": model_config,\n",
    "                    }, os.path.join(checkpoint_path, \"pytorch_model.bin\"))\n",
    "\n",
    "                    tokenizer.save_pretrained(checkpoint_path)\n",
    "                    logger.info(f\"Checkpoint saved at step {global_step}\")\n",
    "\n",
    "                # Check if max steps reached\n",
    "                if config.max_steps and global_step >= config.max_steps:\n",
    "                    break\n",
    "\n",
    "        if config.max_steps and global_step >= config.max_steps:\n",
    "            break\n",
    "\n",
    "    # Save final model\n",
    "    if is_main_process:\n",
    "        final_path = os.path.join(config.output_dir, \"final\")\n",
    "        os.makedirs(final_path, exist_ok=True)\n",
    "\n",
    "        model_to_save = model.module if hasattr(model, \"module\") else model\n",
    "        torch.save({\n",
    "            \"model\": model_to_save.state_dict(),\n",
    "            \"config\": model_config,\n",
    "        }, os.path.join(final_path, \"pytorch_model.bin\"))\n",
    "\n",
    "        tokenizer.save_pretrained(final_path)\n",
    "        logger.info(f\"Final model saved to {final_path}\")\n",
    "\n",
    "        if config.use_wandb:\n",
    "            wandb.finish()\n",
    "\n",
    "    if world_size > 1:\n",
    "        dist.destroy_process_group()\n"
   ],
   "id": "225b19cee452c184",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "config = TrainingConfig()\n",
    "train(config)\n",
    "\n"
   ],
   "id": "7f6e3cf9b87523cf"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
